{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment: Ensemble Methods: DSEI210-Fall-2019: Michael Grossberg\n",
    "\n",
    "In this assignment we are going to get comfortable combining and leveraging ML algorithms with ensemble methods. In order to get respectable results we would more certainly need to spend much more time on data normalization and cleaning than we will be able to. Also we are having you subsample the data severly (so it will run in a reasonable amount of time) which will have a strong negative effect on classification.\n",
    "\n",
    "**Background:** Suppose you are given a collection of high-importance dermatoscopic images of skin lesions. With a high degree of certainty, you want to determine whether or not a particular lesion is malignant or benign. \n",
    "\n",
    "**Objective:** Experiment with different ensemble paradigms combined with different classifiers. See if ensembles help and how. How do they effect accruacy, precision, recall, f1 and variance vs. bias.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Introductory Data Analysis and Preprocessing\n",
    "\n",
    "The path here will be to go from data and meta data to a standard matrix 'X' where the rows are observations, and the columns are features, in this case pixels. We also have a 'y' which is a boolean '0' for a benign kind of cancer and '1' for a malignant kind.\n",
    "\n",
    "### My Imports\n",
    "\n",
    "Here are the extensive list of imports for the whole notbook I had. You may have some differences in your solution. There are other ways to resize and import images. If you prefer those you are free to use them but you should, at least, know how to use these libraries. It is a very good idea to keep constants that are frequently used in your data at the TOP of the file. This is starndard across software development. In particular don't hide data paths deep in your code. When you get the repo there will be a zip file for the data. Unzip that file wherever you want (usually outside your your repo) but use the path variable to point to that location (here called `DATA_PATH`). You will also\n",
    "come up with a location for `temp` data we called `PROCESSED_DATA_PATH`. This is where you save out your preprocessed images and the data for your `X` and `y`. This is important because you may find you have to kill your kernel, restart some other time and you want to save your work so you can resume.\n",
    "\n",
    "<div></div>\n",
    "<hr/>\n",
    "<div style=\"background-color:azure;color:red\"> <em>Tip:</em> Some of the algorithms run really slow. For debugging purposes only, run the algorithms on 1/4 or 1/8 of the data. Then double it and measure the running times. Before you run the bagging algorithms you should have some idea how long it will take to run the whole algorithm. Keep that in mind when you change parameters and explore better performance. Also I have included an import for FloatProgress. Learn how to use that, particularly in the preprocessing, so you know how far you are at any time.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Stuff and Pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as op\n",
    "import json\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from ipywidgets import FloatProgress\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Fix imbalanced data sets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# This is the relative path where the data is stored\n",
    "DATA_PATH = \"./data/SampledImages\"\n",
    "PROCESSED_DATA_PATH = \"./data/processed_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip the dataset\n",
    "\n",
    "Unzip the data set from the repo or download it. Put the SampledImages directory in some location and change the data path accordingly. You will see two kinds of files. One kind of file end with \".jpeg\" and the other have no extension. \n",
    "\n",
    "* Flip through the images manually\n",
    "* We will create a directory to hold processed images which we will rite to disk and stored in `PROCESSED_DATA_PATH`.\n",
    "* Use the os and the os.path python library to check if the directory exists and if not create it\n",
    "* You can loop through the `DATA_PATH` by reading the contents of the directory with `os.listdir` and running a for loop. We will treat the `jpeg` files and the metadata files separately. Keep in mind that we will build our `X` matrix of classification input from our images and our `y` classification outputs from our metadata.\n",
    "* Lets focus on the metadata first. This meta data is in JSON form so you can parse them using the python json library imported above. You can either read them in as text and then use the json library to parse and convert it to a dictionary, or read it directly in as a JSON file. For each file we will look for a sub-key with name `benign_malignant`. If that key is present and the key has the value `malignant` the corresponding target value in `y` should be 1. A small number of rows are missing this key. In that case check the `diagnosis` key. In this case if the `diagnosis` is `basal cell carcinoma` then `y` should also be 1 otherwise, for other `diagnosis` when there is no `benign_malignant` key. Finally if there is no `benign_malignant` and no `diagnosis` value, the meta-data and corresponding image, must be ignored.\n",
    "* In order to make the problem tractable you will read in an image using imread, resize the images to 25x25 pixels and store the image as a row in a matrix which will have one row for each image and 25x25x3 columns with 3 being R-G-B color channels. It is also worthwile to save out the images using imsave with a slightly different file name (like tacking on `small` before the `.jpeg`) in your `PROCESSED_DATA_PATH` directory. When you have created a matrix `X` with one row for each image for which we have a meta-data value 0 or 1 in `y`, save both `X` and `y` in the numpy file format (.npy) usin the command `np.save`. Keep in mind that we should check that this file(s) exists first before processing the images each time you run the notebook. This way you can start and stop your analysis loading from the file, if you processed the data or recomputing if you don't have them proceessed. You can manaually override the test when you need to so it will compute everyting again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    \"\"\"\n",
    "    Returns two arrays: \n",
    "        X is an array of resized images\n",
    "        y is an array of labels\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    file_names = list(map(lambda x: DATA_PATH + '/' + x,sorted(os.listdir(DATA_PATH))))\n",
    "    \n",
    "    if not os.path.exists(PROCESSED_DATA_PATH):\n",
    "        os.makedirs(PROCESSED_DATA_PATH)\n",
    "\n",
    "    for i in range(0, len(file_names), 2):\n",
    "        file = None\n",
    "        with open(file_names[i]) as f:\n",
    "            file = json.loads(f.read())\n",
    "        if 'benign_malignant' in file['meta']['clinical'] and file['meta']['clinical']['benign_malignant']:\n",
    "            if file['meta']['clinical']['benign_malignant'] == 'malignant':\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "        else:\n",
    "            if 'diagnosis' in file['meta']['clinical'] and file['meta']['clinical']['diagnosis']:\n",
    "                if file['meta']['clinical']['diagnosis'] == 'basal cell carcinoma':\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "            else:\n",
    "                 continue\n",
    "\n",
    "        img = imread(file_names[i+1])\n",
    "        img_resize = resize(img, (25,25,3))\n",
    "        new_file_name = file_names[i+1].replace(\"SampledImages\", \"processed_images\").replace(\".jpeg\",\"_small.jpeg\")\n",
    "        imsave(new_file_name, img_resize)\n",
    "\n",
    "        X.append(img_resize)\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "ISIC_images_X_savefile = './ISIC_images_X.npy'\n",
    "ISIC_images_y_savefile = './ISIC_images_y.npy'\n",
    "\n",
    "ISIC_images_not_cached = not op.exists(ISIC_images_X_savefile) and not op.exists(ISIC_images_y_savefile)\n",
    "print(ISIC_images_not_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISIC_images_not_cached:\n",
    "    X, y = load_images()\n",
    "    \n",
    "    np.save(\"ISIC_images_X.npy\", X)\n",
    "    np.save(\"ISIC_images_y.npy\", y)\n",
    "else:\n",
    "    X = np.load(ISIC_images_X_savefile)\n",
    "    y = np.load(ISIC_images_y_savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1527\n",
       "1     239\n",
       "dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(list(y)).value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our training and testing sets\n",
    "\n",
    "At this point you should have `X` and `y` loaded. I stored them, combined in a single file called `data_array.npy`. Compute the count of malignant and benign by counting and printing the frequency of the two classes. You will see that the benign class is much more frequent. As a result we are going to consider using sythetic minority oversampling to deal with the imbalenced class. You need to install the `imbalanced-learn` module conda install. We will use the *synthetic minority over-sampling* within that library for that task. Below is our train test split and resampling application. What you see below is our usual train test split, followed by a resampled train test split using SMOTE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frquency of malignant:0.13533408833522084\n",
      "The frquency of benign:0.8646659116647791\n"
     ]
    }
   ],
   "source": [
    "malignant_frequency = sum(y) / len(y)\n",
    "print(f\"The frquency of malignant:{malignant_frequency}\")\n",
    "print(f\"The frquency of benign:{1-malignant_frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny, nc = np.array(X).shape\n",
    "X_dim_2 = np.array(X).reshape(nsamples, nx*ny*nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dim_2, y, test_size=0.3, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "X_resamp_train, y_resamp_train = sm.fit_sample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 1875)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For reproducible results, througout we will include `random_state=42` as a parameter\n",
    "to all possible funtions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Evaluation\n",
    "\n",
    "In this section we will first apply some basic classifiers to understand the speed and baseline performance results we should expect. We will start out with basic training test splits. Note that because we are not using crossvalidation at the start, we should expect that our testing results might be overly optimistic.\n",
    "\n",
    "### 1. How well does the resampling work?\n",
    "\n",
    "For our evaluation we will focus on 4 metrics. We note that we don't really have to worry about multiple classes since this is a binary classification problem. Thus we only look at the four metrics:\n",
    "\n",
    " * f1\n",
    " * accuracy\n",
    " * precision\n",
    " * recall\n",
    " \n",
    "We look at this for training and test. You may find it convienient to create boilerplate code for this within a loop because we will be doing this kind of evalution many times. For our first experiment just look at\n",
    "\n",
    "* k-Nearest Neighbors\n",
    "\n",
    "In this case we will use k=3. Now compute the four metrics for the unresampled X_train, y_train and X_test, y_test.  For the first set of numbers you *might* see similar results to the following:\n",
    "\n",
    "    Regular KNN N=3 neighbors\n",
    "    accuracy_score on train  fit: \t     0.882\n",
    "    f1_score  on train  fit: \t         0.61\n",
    "    precision_score on train  fit: \t     0.884\n",
    "    recall_score on train  fit: \t     0.466\n",
    "    accuracy_score on test  fit: \t     0.805\n",
    "    f1_score  on test  fit: \t         0.368\n",
    "    precision_score on test  fit: \t     0.51\n",
    "    recall_score on test  fit: \t         0.287\n",
    "\n",
    "Consider recall and review the score. A 0.287 score means that for 100% of real malignant cases **only** 29% are flagged as being malignant. While a higher than desirable false alarms (lower precision score) might be tolerable because patents may be retested, low recall is not acceptable.  Next do this again, separately where training takes place on X_resamp_train, y_resamp_train but **test** on X_test, y_test and evaluate with the 4 metric. With this mixed training how does KNN with N=3 perform accross the training and testing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular KNN N=3 neighbors\n",
      "For train dataset:\n",
      "accuracy_score on train  fit: 0.888350\n",
      "f1_score on train  fit: 0.372727\n",
      "precision_score on train  fit: 0.854167\n",
      "recall_score on train  fit: 0.238372\n",
      "For test dataset:\n",
      "accuracy_score on test  fit: 0.866038\n",
      "f1_score on test  fit: 0.101266\n",
      "precision_score on test  fit: 0.333333\n",
      "recall_score on test  fit: 0.059701\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_predict_train = knn.predict(X_train)\n",
    "y_predict_test = knn.predict(X_test)\n",
    "\n",
    "print(\"Regular KNN N=3 neighbors\")\n",
    "print(\"For train dataset:\")\n",
    "print(\"accuracy_score on train  fit: %f\" % accuracy_score(y_train, y_predict_train))\n",
    "print(\"f1_score on train  fit: %f\" % f1_score(y_train, y_predict_train))\n",
    "print(\"precision_score on train  fit: %f\" % precision_score(y_train, y_predict_train))\n",
    "print(\"recall_score on train  fit: %f\" % recall_score(y_train, y_predict_train))\n",
    "print(\"For test dataset:\")\n",
    "print(\"accuracy_score on test  fit: %f\" % accuracy_score(y_test, y_predict_test))\n",
    "print(\"f1_score on test  fit: %f\" % f1_score(y_test, y_predict_test))\n",
    "print(\"precision_score on test  fit: %f\" % precision_score(y_test, y_predict_test))\n",
    "print(\"recall_score on test  fit: %f\" % recall_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular KNN N=3 neighbors\n",
      "For train dataset:\n",
      "accuracy_score on train  fit: 0.923872\n",
      "f1_score on train  fit: 0.929134\n",
      "precision_score on train  fit: 0.869067\n",
      "recall_score on train  fit: 0.998120\n",
      "For test dataset:\n",
      "accuracy_score on test  fit: 0.747170\n",
      "f1_score on test  fit: 0.294737\n",
      "precision_score on test  fit: 0.227642\n",
      "recall_score on test  fit: 0.417910\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit( X_resamp_train,  y_resamp_train)\n",
    "y_predict_train = knn.predict( X_resamp_train)\n",
    "y_predict_test = knn.predict(X_test)\n",
    "\n",
    "print(\"Regular KNN N=3 neighbors\")\n",
    "print(\"For train dataset:\")\n",
    "print(\"accuracy_score on train  fit: %f\" % accuracy_score(y_resamp_train, y_predict_train))\n",
    "print(\"f1_score on train  fit: %f\" % f1_score(y_resamp_train, y_predict_train))\n",
    "print(\"precision_score on train  fit: %f\" % precision_score(y_resamp_train, y_predict_train))\n",
    "print(\"recall_score on train  fit: %f\" % recall_score(y_resamp_train, y_predict_train))\n",
    "print(\"For test dataset:\")\n",
    "print(\"accuracy_score on test  fit: %f\" % accuracy_score(y_test, y_predict_test))\n",
    "print(\"f1_score on test  fit: %f\" % f1_score(y_test, y_predict_test))\n",
    "print(\"precision_score on test  fit: %f\" % precision_score(y_test, y_predict_test))\n",
    "print(\"recall_score on test  fit: %f\" % recall_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basline with training on resampled data and testing with test data\n",
    "\n",
    "We are now going to consider a sequence of algorithms and then ensemble versions of these algorithms. Because we will be using a number of algorithms and comparing their results you will want to write a loop. To strictly minimize copy past code and instead use template strings and set up your classifiers in an array so you can loop through. In this part you neet to use `cross_validate` from `sklearn.model_selection`. You are going to be calling this on the resampled data which has balanced classes. You should do 5-fold crossvalidation, you should make sure that it displays all the scoreing functions `scoring=['f1','accuracy','precision','recall']`, that it returns the training scores `return_train_score=True` and that you stratify using the `groups` optional argument. You are making your own reports here, where you print out the training and testing scores for each of the 4 metrics. Because `cross_validate` returns a score for each fold of the cross validataion, what you should show is the mean and the standard devation. We basically assume that the standard devation is the \"error\" in our estimate so for training accruacy you it might print `0.92 +/- 0.01` where the first number is the mean of the 5 accuracies for that classifier, and the second number is the stdev. Do this for these 4 classifiers:\n",
    "\n",
    "1. Random Forest\n",
    "2. k-Nearest Neighbors \n",
    "3. Extra Trees\n",
    "4. Support Vector Machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_f1', 'train_f1', 'test_accuracy', 'train_accuracy', 'test_precision', 'train_precision', 'test_recall', 'train_recall'])\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator=knn,\n",
    "                         X=X_resamp_train,\n",
    "                         y=y_resamp_train,\n",
    "                         cv=5,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "print(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 0.92 (+/- 0.00) [KNN]\n",
      "Accuracy Test: 0.87 (+/- 0.01) [KNN]\n",
      "F1 Train: 0.92 (+/- 0.00) [KNN]\n",
      "F1 Test: 0.89 (+/- 0.01) [KNN]\n",
      "Precision Train: 0.86 (+/- 0.01) [KNN]\n",
      "Precision Test: 0.80 (+/- 0.02) [KNN]\n",
      "Recall Train: 1.00 (+/- 0.00) [KNN]\n",
      "Recall Test: 1.00 (+/- 0.01) [KNN]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_accuracy']), np.std(scores['train_accuracy']), \"KNN\"))\n",
    "print(\"Accuracy Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']), \"KNN\"))\n",
    "print(\"F1 Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_f1']), np.std(scores['train_f1']), \"KNN\"))\n",
    "print(\"F1 Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_f1']), np.std(scores['test_f1']), \"KNN\"))\n",
    "print(\"Precision Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_precision']), np.std(scores['train_precision']), \"KNN\"))\n",
    "print(\"Precision Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_precision']), np.std(scores['test_precision']), \"KNN\"))\n",
    "print(\"Recall Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_recall']), np.std(scores['train_recall']), \"KNN\"))\n",
    "print(\"Recall Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_recall']), np.std(scores['test_recall']), \"KNN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy Train: 1.00 (+/- 0.00) [Random Forest]\n",
      "Accuracy Test: 0.90 (+/- 0.03) [Random Forest]\n",
      "F1 Train: 1.00 (+/- 0.00) [Random Forest]\n",
      "F1 Test: 0.90 (+/- 0.03) [Random Forest]\n",
      "Precision Train: 1.00 (+/- 0.00) [Random Forest]\n",
      "Precision Test: 0.89 (+/- 0.03) [Random Forest]\n",
      "Recall Train: 1.00 (+/- 0.00) [Random Forest]\n",
      "Recall Test: 0.92 (+/- 0.04) [Random Forest]\n",
      "Accuracy Train: 0.92 (+/- 0.00) [k-Nearest Neighbors]\n",
      "Accuracy Test: 0.87 (+/- 0.01) [k-Nearest Neighbors]\n",
      "F1 Train: 0.92 (+/- 0.00) [k-Nearest Neighbors]\n",
      "F1 Test: 0.89 (+/- 0.01) [k-Nearest Neighbors]\n",
      "Precision Train: 0.86 (+/- 0.01) [k-Nearest Neighbors]\n",
      "Precision Test: 0.80 (+/- 0.02) [k-Nearest Neighbors]\n",
      "Recall Train: 1.00 (+/- 0.00) [k-Nearest Neighbors]\n",
      "Recall Test: 1.00 (+/- 0.01) [k-Nearest Neighbors]\n",
      "Accuracy Train: 1.00 (+/- 0.00) [Extra Trees]\n",
      "Accuracy Test: 0.82 (+/- 0.02) [Extra Trees]\n",
      "F1 Train: 1.00 (+/- 0.00) [Extra Trees]\n",
      "F1 Test: 0.82 (+/- 0.02) [Extra Trees]\n",
      "Precision Train: 1.00 (+/- 0.00) [Extra Trees]\n",
      "Precision Test: 0.79 (+/- 0.01) [Extra Trees]\n",
      "Recall Train: 1.00 (+/- 0.00) [Extra Trees]\n",
      "Recall Test: 0.86 (+/- 0.02) [Extra Trees]\n",
      "Accuracy Train: 0.75 (+/- 0.01) [Support Vector Machine]\n",
      "Accuracy Test: 0.72 (+/- 0.02) [Support Vector Machine]\n",
      "F1 Train: 0.74 (+/- 0.01) [Support Vector Machine]\n",
      "F1 Test: 0.71 (+/- 0.02) [Support Vector Machine]\n",
      "Precision Train: 0.75 (+/- 0.00) [Support Vector Machine]\n",
      "Precision Test: 0.72 (+/- 0.03) [Support Vector Machine]\n",
      "Recall Train: 0.74 (+/- 0.02) [Support Vector Machine]\n",
      "Recall Test: 0.70 (+/- 0.03) [Support Vector Machine]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "et = ExtraTreeClassifier(random_state=42)\n",
    "\n",
    "svm = SVC(gamma='auto', random_state=42)\n",
    "\n",
    "\n",
    "clf_labels = ['Random Forest', 'k-Nearest Neighbors', 'Extra Trees', 'Support Vector Machine']\n",
    "scoring=['f1','accuracy','precision','recall']\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([rf, knn, et, svm], clf_labels):\n",
    "    scores = cross_validate(estimator=clf,\n",
    "                             X=X_resamp_train,\n",
    "                             y=y_resamp_train,\n",
    "                             cv=5,\n",
    "                             scoring=scoring,\n",
    "                             return_train_score=True)\n",
    "    print(\"Accuracy Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_accuracy']), np.std(scores['train_accuracy']), label))\n",
    "    print(\"Accuracy Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']), label))\n",
    "    print(\"F1 Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_f1']), np.std(scores['train_f1']), label))\n",
    "    print(\"F1 Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_f1']), np.std(scores['test_f1']), label))\n",
    "    print(\"Precision Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_precision']), np.std(scores['train_precision']), label))\n",
    "    print(\"Precision Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_precision']), np.std(scores['test_precision']), label))\n",
    "    print(\"Recall Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_recall']), np.std(scores['train_recall']), label))\n",
    "    print(\"Recall Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_recall']), np.std(scores['test_recall']), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bagging the classiefiers\n",
    "\n",
    "So here we are going to see if bagging the classifiers can help. You are going to run the same 4 classifiers through but wrap them in a `BaggingClassifier` which will use boostrap resampling. You also will should start by trying 60% (.6) of the sampling data, and only 70% (.7) of the features to make each of the ensemble learners more independent. Start out with 20 estimators. Since you need to pass this config to each of the constructors you can make a dictionary of config variables and then use the python `**` to unpack it. So if the config variables are like this:\n",
    "\n",
    "`bagged_config = dict(n_estimators=20, max_samples=.6, max_features=.7, random_state=42)`\n",
    "\n",
    "Then you use them like this:\n",
    "\n",
    "`BaggingClassifier(KNeighborsClassifier(n_neighbors=3), **bagged_config)`\n",
    "\n",
    "Write this as a loop over the classiffiers like above but for debuging purposes you can use the python `break` statement to only do one loop until you get it working. Each time through it should print the report like you made above shouing training and testing of each of the four matrics, using 5-fold cross_validataion which gives you a mean and a standard devation just like above. Compare the performance. Try adjusting the parameters such as max_samples or max_features to improve the performance.\n",
    "\n",
    "Explain for each of the 4 bagged versions of your classifiers:\n",
    "\n",
    "1. Random Forest\n",
    "2. k-Nearest Neighbors \n",
    "3. Extra Trees\n",
    "4. Support Vector Machine\n",
    "\n",
    "explain why the results make sense or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy Train: 0.98 (+/- 0.00) [Bagging Random Forest]\n",
      "Accuracy Test: 0.89 (+/- 0.02) [Bagging Random Forest]\n",
      "F1 Train: 0.98 (+/- 0.00) [Bagging Random Forest]\n",
      "F1 Test: 0.89 (+/- 0.02) [Bagging Random Forest]\n",
      "Precision Train: 0.97 (+/- 0.00) [Bagging Random Forest]\n",
      "Precision Test: 0.84 (+/- 0.02) [Bagging Random Forest]\n",
      "Recall Train: 0.99 (+/- 0.00) [Bagging Random Forest]\n",
      "Recall Test: 0.95 (+/- 0.03) [Bagging Random Forest]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-9528f7c3179f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                              \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                              \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                              return_train_score=True)\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy Train: %0.2f (+/- %0.2f) [%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy Test: %0.2f (+/- %0.2f) [%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             train_scores = _score(estimator, X_train, y_train, scorer,\n\u001b[0;32m--> 572\u001b[0;31m                                   is_multimetric)\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \"\"\"\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \"\"\"\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mpredicted_probabilitiy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n\u001b[1;32m    648\u001b[0m                                   axis=0)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                 self.n_classes_)\n\u001b[0;32m--> 695\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_parallel_predict_proba\u001b[0;34m(estimators, estimators_features, X, n_classes)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimators_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_proba\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mproba_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 delayed_query(\n\u001b[1;32m    454\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             )\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, data, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "et = ExtraTreeClassifier(random_state=42)\n",
    "\n",
    "svm = SVC(gamma='auto', random_state=42)\n",
    "\n",
    "bagged_config = dict(n_estimators=20, max_samples=.6, max_features=.7, random_state=42)\n",
    "\n",
    "clf_labels = ['Bagging Random Forest', ' Bagging k-Nearest Neighbors', 'Bagging Extra Trees', 'Bagging Support Vector Machine']\n",
    "scoring=['f1','accuracy','precision','recall']\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([rf, knn, et, svm], clf_labels):\n",
    "    bc = BaggingClassifier(clf, **bagged_config)\n",
    "\n",
    "\n",
    "    scores = cross_validate(estimator=bc,\n",
    "                             X=X_resamp_train,\n",
    "                             y=y_resamp_train,\n",
    "                             cv=5,\n",
    "                             scoring=scoring,\n",
    "                             bootstrap=True,\n",
    "                             return_train_score=True)\n",
    "    \n",
    "    print(\"Accuracy Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_accuracy']), np.std(scores['train_accuracy']), label))\n",
    "    print(\"Accuracy Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']), label))\n",
    "    print(\"F1 Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_f1']), np.std(scores['train_f1']), label))\n",
    "    print(\"F1 Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_f1']), np.std(scores['test_f1']), label))\n",
    "    print(\"Precision Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_precision']), np.std(scores['train_precision']), label))\n",
    "    print(\"Precision Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_precision']), np.std(scores['test_precision']), label))\n",
    "    print(\"Recall Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_recall']), np.std(scores['train_recall']), label))\n",
    "    print(\"Recall Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_recall']), np.std(scores['test_recall']), label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adaptive Boosting\n",
    "\n",
    "Here we will consider the adaptive boosting classifier. Not all learners can be boosted (unlike bagging). We will only consider two boosted algorithms here. \n",
    "\n",
    "1. Extra Tree\n",
    "2. Descision Tree\n",
    "\n",
    "In the case of decision tree use max_depth=2 and max_features .7. Here try 100 estimators in the `AdaBoostClassifier` wrapping the two algorothms. Just like above use 5 fold cross_validataion, random_seed=42 everywhere, and print out your reports to be able to compare. Play with the paramters a bit to try to get the best results possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 1.00 (+/- 0.00) [Adaptive Boosting Decision Tree]\n",
      "Accuracy Test: 0.92 (+/- 0.03) [Adaptive Boosting Decision Tree]\n",
      "F1 Train: 1.00 (+/- 0.00) [Adaptive Boosting Decision Tree]\n",
      "F1 Test: 0.92 (+/- 0.03) [Adaptive Boosting Decision Tree]\n",
      "Precision Train: 1.00 (+/- 0.00) [Adaptive Boosting Decision Tree]\n",
      "Precision Test: 0.89 (+/- 0.02) [Adaptive Boosting Decision Tree]\n",
      "Recall Train: 1.00 (+/- 0.00) [Adaptive Boosting Decision Tree]\n",
      "Recall Test: 0.96 (+/- 0.04) [Adaptive Boosting Decision Tree]\n",
      "Accuracy Train: 1.00 (+/- 0.00) [Adaptive Boosting Extra Trees]\n",
      "Accuracy Test: 0.83 (+/- 0.02) [Adaptive Boosting Extra Trees]\n",
      "F1 Train: 1.00 (+/- 0.00) [Adaptive Boosting Extra Trees]\n",
      "F1 Test: 0.84 (+/- 0.02) [Adaptive Boosting Extra Trees]\n",
      "Precision Train: 1.00 (+/- 0.00) [Adaptive Boosting Extra Trees]\n",
      "Precision Test: 0.80 (+/- 0.02) [Adaptive Boosting Extra Trees]\n",
      "Recall Train: 1.00 (+/- 0.00) [Adaptive Boosting Extra Trees]\n",
      "Recall Test: 0.88 (+/- 0.02) [Adaptive Boosting Extra Trees]\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=2, max_features = .7 ,random_state=42)\n",
    "et = ExtraTreeClassifier(random_state=42)\n",
    "\n",
    "clf_labels = ['Adaptive Boosting Decision Tree', 'Adaptive Boosting Extra Trees']\n",
    "scoring=['f1','accuracy','precision','recall']\n",
    "\n",
    "\n",
    "for clf, label in zip([dtc, et], clf_labels):\n",
    "    abc = AdaBoostClassifier(base_estimator=clf, n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "    scores = cross_validate(estimator=abc,\n",
    "                                 X=X_resamp_train,\n",
    "                                 y=y_resamp_train,\n",
    "                                 cv=5,\n",
    "                                 scoring=scoring,\n",
    "                                 return_train_score=True)\n",
    "\n",
    "    print(\"Accuracy Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_accuracy']), np.std(scores['train_accuracy']), label))\n",
    "    print(\"Accuracy Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']), label))\n",
    "    print(\"F1 Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_f1']), np.std(scores['train_f1']), label))\n",
    "    print(\"F1 Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_f1']), np.std(scores['test_f1']), label))\n",
    "    print(\"Precision Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_precision']), np.std(scores['train_precision']), label))\n",
    "    print(\"Precision Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_precision']), np.std(scores['test_precision']), label))\n",
    "    print(\"Recall Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_recall']), np.std(scores['train_recall']), label))\n",
    "    print(\"Recall Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_recall']), np.std(scores['test_recall']), label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting\n",
    "\n",
    "Below do exactly as above with Adaptive boosting but now with gradient boosting. Unfortunately you will not be able to use ExtraTrees here so you will end up just gradient boosting the default Decision Tree classifier. Again report the results and as before do 5 fold cross valitation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 1.00 (+/- 0.00) [Gradient Boosting Descision Tree]\n",
      "Accuracy Test: 0.92 (+/- 0.02) [Gradient Boosting Descision Tree]\n",
      "F1 Train: 1.00 (+/- 0.00) [Gradient Boosting Descision Tree]\n",
      "F1 Test: 0.92 (+/- 0.02) [Gradient Boosting Descision Tree]\n",
      "Precision Train: 0.99 (+/- 0.00) [Gradient Boosting Descision Tree]\n",
      "Precision Test: 0.89 (+/- 0.01) [Gradient Boosting Descision Tree]\n",
      "Recall Train: 1.00 (+/- 0.00) [Gradient Boosting Descision Tree]\n",
      "Recall Test: 0.96 (+/- 0.03) [Gradient Boosting Descision Tree]\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "label = \"Gradient Boosting Descision Tree\"\n",
    "scores = cross_validate(estimator=gbc,\n",
    "                             X=X_resamp_train,\n",
    "                             y=y_resamp_train,\n",
    "                             cv=5,\n",
    "                             scoring=scoring,\n",
    "                             return_train_score=True)\n",
    "\n",
    "print(\"Accuracy Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_accuracy']), np.std(scores['train_accuracy']), label))\n",
    "print(\"Accuracy Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']), label))\n",
    "print(\"F1 Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_f1']), np.std(scores['train_f1']), label))\n",
    "print(\"F1 Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_f1']), np.std(scores['test_f1']), label))\n",
    "print(\"Precision Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_precision']), np.std(scores['train_precision']), label))\n",
    "print(\"Precision Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_precision']), np.std(scores['test_precision']), label))\n",
    "print(\"Recall Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_recall']), np.std(scores['train_recall']), label))\n",
    "print(\"Recall Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_recall']), np.std(scores['test_recall']), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Voting classifier\n",
    "\n",
    "Here you are going to implement a card voting classifier. You will pick three diverse learners of your choice. For a group of learners to be diverse means that they make different errors on the same data. In other words, one might say that the errors made by the classifers are uncorrelated.\n",
    "\n",
    "Use sklearn's VotingClassifier to bind them together and again use cross_validate to score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 1.00 (+/- 0.00) [Hard Voting]\n",
      "Accuracy Test: 0.90 (+/- 0.01) [Hard Voting]\n",
      "F1 Train: 1.00 (+/- 0.00) [Hard Voting]\n",
      "F1 Test: 0.90 (+/- 0.01) [Hard Voting]\n",
      "Precision Train: 1.00 (+/- 0.00) [Hard Voting]\n",
      "Precision Test: 0.85 (+/- 0.01) [Hard Voting]\n",
      "Recall Train: 1.00 (+/- 0.00) [Hard Voting]\n",
      "Recall Test: 0.97 (+/- 0.02) [Hard Voting]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "et = ExtraTreeClassifier(random_state=42)\n",
    "\n",
    "estimators = [('rf', rf), ('knn', knn), (\"et\", et)]\n",
    "\n",
    "vc = VotingClassifier(estimators=estimators)\n",
    "\n",
    "label = \"Hard Voting\"\n",
    "scores = cross_validate(estimator=vc,\n",
    "                             X=X_resamp_train,\n",
    "                             y=y_resamp_train,\n",
    "                             cv=5,\n",
    "                             scoring=scoring,\n",
    "                             return_train_score=True)\n",
    "\n",
    "print(\"Accuracy Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_accuracy']), np.std(scores['train_accuracy']), label))\n",
    "print(\"Accuracy Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy']), label))\n",
    "print(\"F1 Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_f1']), np.std(scores['train_f1']), label))\n",
    "print(\"F1 Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_f1']), np.std(scores['test_f1']), label))\n",
    "print(\"Precision Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_precision']), np.std(scores['train_precision']), label))\n",
    "print(\"Precision Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_precision']), np.std(scores['test_precision']), label))\n",
    "print(\"Recall Train: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['train_recall']), np.std(scores['train_recall']), label))\n",
    "print(\"Recall Test: %0.2f (+/- %0.2f) [%s]\" % (np.mean(scores['test_recall']), np.std(scores['test_recall']), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Write a conclusion explaining what we should take away from the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowlegement: Many Thanks to Toma Suciu for the first draft**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is time-consuming to use cross_validate()\n",
    "\n",
    "2. Using VotingClassifier() can improve recall if the leaners in estimators are not bad.\n",
    "\n",
    "3. The Recall is 1 when using knn. This is awsome"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
