{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment: Ensemble Methods: DSEI210-Fall-2019: Michael Grossberg\n",
    "\n",
    "In this assignment we are going to get comfortable combining and leveraging ML algorithms with ensemble methods. In order to get respectable results we would more certainly need to spend much more time on data normalization and cleaning than we will be able to. Also we are having you subsample the data severly (so it will run in a reasonable amount of time) which will have a strong negative effect on classification.\n",
    "\n",
    "**Background:** Suppose you are given a collection of high-importance dermatoscopic images of skin lesions. With a high degree of certainty, you want to determine whether or not a particular lesion is malignant or benign. \n",
    "\n",
    "**Objective:** Experiment with different ensemble paradigms combined with different classifiers. See if ensembles help and how. How do they effect accruacy, precision, recall, f1 and variance vs. bias.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Introductory Data Analysis and Preprocessing\n",
    "\n",
    "The path here will be to go from data and meta data to a standard matrix 'X' where the rows are observations, and the columns are features, in this case pixels. We also have a 'y' which is a boolean '0' for a benign kind of cancer and '1' for a malignant kind.\n",
    "\n",
    "### My Imports\n",
    "\n",
    "Here are the extensive list of imports for the whole notbook I had. You may have some differences in your solution. There are other ways to resize and import images. If you prefer those you are free to use them but you should, at least, know how to use these libraries. It is a very good idea to keep constants that are frequently used in your data at the TOP of the file. This is starndard across software development. In particular don't hide data paths deep in your code. When you get the repo there will be a zip file for the data. Unzip that file wherever you want (usually outside your your repo) but use the path variable to point to that location (here called `DATA_PATH`). You will also\n",
    "come up with a location for `temp` data we called `PROCESSED_DATA_PATH`. This is where you save out your preprocessed images and the data for your `X` and `y`. This is important because you may find you have to kill your kernel, restart some other time and you want to save your work so you can resume.\n",
    "\n",
    "<div></div>\n",
    "<hr/>\n",
    "<div style=\"background-color:azure;color:red\"> <em>Tip:</em> Some of the algorithms run really slow. For debugging purposes only, run the algorithms on 1/4 or 1/8 of the data. Then double it and measure the running times. Before you run the bagging algorithms you should have some idea how long it will take to run the whole algorithm. Keep that in mind when you change parameters and explore better performance. Also I have included an import for FloatProgress. Learn how to use that, particularly in the preprocessing, so you know how far you are at any time.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8965a8c02d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Fix imbalanced data sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Machine Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# Standard Stuff and Pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as op\n",
    "import json\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from ipywidgets import FloatProgress\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Fix imbalanced data sets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# This is the relative path where the data is stored\n",
    "DATA_PATH = \"../data/SampledImages\"\n",
    "PROCESSED_DATA_PATH = \"../data/processed_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip the dataset\n",
    "\n",
    "Unzip the data set from the repo or download it. Put the SampledImages directory in some location and change the data path accordingly. You will see two kinds of files. One kind of file end with \".jpeg\" and the other have no extension. \n",
    "\n",
    "* Flip through the images manually\n",
    "* We will create a directory to hold processed images which we will rite to disk and stored in `PROCESSED_DATA_PATH`.\n",
    "* Use the os and the os.path python library to check if the directory exists and if not create it\n",
    "* You can loop through the `DATA_PATH` by reading the contents of the directory with `os.listdir` and running a for loop. We will treat the `jpeg` files and the metadata files separately. Keep in mind that we will build our `X` matrix of classification input from our images and our `y` classification outputs from our metadata.\n",
    "* Lets focus on the metadata first. This meta data is in JSON form so you can parse them using the python json library imported above. You can either read them in as text and then use the json library to parse and convert it to a dictionary, or read it directly in as a JSON file. For each file we will look for a sub-key with name `benign_malignant`. If that key is present and the key has the value `malignant` the corresponding target value in `y` should be 1. A small number of rows are missing this key. In that case check the `diagnosis` key. In this case if the `diagnosis` is `basal cell carcinoma` then `y` should also be 1 otherwise, for other `diagnosis` when there is no `benign_malignant` key. Finally if there is no `benign_malignant` and no `diagnosis` value, the meta-data and corresponding image, must be ignored.\n",
    "* In order to make the problem tractable you will read in an image using imread, resize the images to 25x25 pixels and store the image as a row in a matrix which will have one row for each image and 25x25x3 columns with 3 being R-G-B color channels. It is also worthwile to save out the images using imsave with a slightly different file name (like tacking on `small` before the `.jpeg`) in your `PROCESSED_DATA_PATH` directory. When you have created a matrix `X` with one row for each image for which we have a meta-data value 0 or 1 in `y`, save both `X` and `y` in the numpy file format (.npy) usin the command `np.save`. Keep in mind that we should check that this file(s) exists first before processing the images each time you run the notebook. This way you can start and stop your analysis loading from the file, if you processed the data or recomputing if you don't have them proceessed. You can manaually override the test when you need to so it will compute everyting again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our training and testing sets\n",
    "\n",
    "At this point you should have `X` and `y` loaded. I stored them, combined in a single file called `data_array.npy`. Compute the count of malignant and benign by counting and printing the frequency of the two classes. You will see that the benign class is much more frequent. As a result we are going to consider using sythetic minority oversampling to deal with the imbalenced class. You need to install the `imbalanced-learn` module conda install. We will use the *synthetic minority over-sampling* within that library for that task. Below is our train test split and resampling application. What you see below is our usual train test split, followed by a resampled train test split using SMOTE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For reproducible results, througout we will include `random_state=42` as a parameter\n",
    "to all possible funtions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Evaluation\n",
    "\n",
    "In this section we will first apply some basic classifiers to understand the speed and baseline performance results we should expect. We will start out with basic training test splits. Note that because we are not using crossvalidation at the start, we should expect that our testing results might be overly optimistic.\n",
    "\n",
    "### 1. How well does the resampling work?\n",
    "\n",
    "For our evaluation we will focus on 4 metrics. We note that we don't really have to worry about multiple classes since this is a binary classification problem. Thus we only look at the four metrics:\n",
    "\n",
    " * f1\n",
    " * accuracy\n",
    " * precision\n",
    " * recall\n",
    " \n",
    "We look at this for training and test. You may find it convienient to create boilerplate code for this within a loop because we will be doing this kind of evalution many times. For our first experiment just look at\n",
    "\n",
    "* k-Nearest Neighbors\n",
    "\n",
    "In this case we will use k=3. Now compute the four metrics for the unresampled X_train, y_train and X_test, y_test.  For the first set of numbers you *might* see similar results to the following:\n",
    "\n",
    "    Regular KNN N=3 neighbors\n",
    "    accuracy_score on train  fit: \t     0.882\n",
    "    f1_score  on train  fit: \t         0.61\n",
    "    precision_score on train  fit: \t     0.884\n",
    "    recall_score on train  fit: \t     0.466\n",
    "    accuracy_score on test  fit: \t     0.805\n",
    "    f1_score  on test  fit: \t         0.368\n",
    "    precision_score on test  fit: \t     0.51\n",
    "    recall_score on test  fit: \t         0.287\n",
    "\n",
    "Consider recall and review the score. A 0.287 score means that for 100% of real malignant cases **only** 29% are flagged as being malignant. While a higher than desirable false alarms (lower precision score) might be tolorable because patents may be retested, low recall is not acceptable.  Next do this again, separately where training takes place on X_resamp_train, y_resamp_train but **test** on X_test, y_test and evaluate with the 4 metric. With this mixed training how does KNN with N=3 perform accross the training and testing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basline with training on resampled data and testing with test data\n",
    "\n",
    "We are now going to consider a sequence of algorithms and then ensemble versions of these algorithms. Because we will be using a number of algorithms and comparing their results you will want to write a loop. To strictly minimize copy past code and instead use template strings and set up your classifiers in an array so you can loop through. In this part you neet to use `cross_validate` from `sklearn.model_selection`. You are going to be calling this on the resampled data which has balanced classes. You should do 5-fold crossvalidation, you should make sure that it displays all the scoreing functions `scoring=['f1','accuracy','precision','recall']`, that it returns the training scores `return_train_score=True` and that you stratify using the `groups` optional argument. You are making your own reports here, where you print out the training and testing scores for each of the 4 metrics. Because `cross_validate` returns a score for each fold of the cross validataion, what you should show is the mean and the standard devation. We basically assume that the standard devation is the \"error\" in our estimate so for training accruacy you it might print `0.92 +/- 0.01` where the first number is the mean of the 5 accuracies for that classifier, and the second number is the stdev. Do this for these 4 classifiers:\n",
    "\n",
    "1. Random Forest\n",
    "2. k-Nearest Neighbors \n",
    "3. Extra Trees\n",
    "4. Support Vector Machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bagging the classiefiers\n",
    "\n",
    "So here we are going to see if bagging the classifiers can help. You are going to run the same 4 classifiers through but wrap them in a `BaggingClassifier` which will use boostrap resampling. You also will should start by trying 60% (.6) of the sampling data, and only 70% (.7) of the features to make each of the ensemble learners more independent. Start out with 20 estimators. Since you need to pass this config to each of the constructors you can make a dictionary of config variables and then use the python `**` to unpack it. So if the config variables are like this:\n",
    "\n",
    "`bagged_config = dict(n_estimators=20, max_samples=.6, max_features=.7, random_state=42)`\n",
    "\n",
    "Then you use them like this:\n",
    "\n",
    "`BaggingClassifier(KNeighborsClassifier(n_neighbors=3), **bagged_config)`\n",
    "\n",
    "Write this as a loop over the classiffiers like above but for debuging purposes you can use the python `break` statement to only do one loop until you get it working. Each time through it should print the report like you made above shouing training and testing of each of the four matrics, using 5-fold cross_validataion which gives you a mean and a standard devation just like above. Compare the performance. Try adjusting the parameters such as max_samples or max_features to improve the performance.\n",
    "\n",
    "Explain for each of the 4 bagged versions of your classifiers:\n",
    "\n",
    "1. Random Forest\n",
    "2. k-Nearest Neighbors \n",
    "3. Extra Trees\n",
    "4. Support Vector Machine\n",
    "\n",
    "explain why the results make sense or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adaptive Boosting\n",
    "\n",
    "Here we will consider the adaptive boosting classifier. Not all learners can be boosted (unlike bagging). We will only consider two boosted algorithms here. \n",
    "\n",
    "1. Extra Tree\n",
    "2. Descision Tree\n",
    "\n",
    "In the case of decision tree use max_depth=2 and max_features .7. Here try 100 estimators in the `AdaBoostClassifier` wrapping the two algorothms. Just like above use 5 fold cross_validataion, random_seed=42 everywhere, and print out your reports to be able to compare. Play with the paramters a bit to try to get the best results possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting\n",
    "\n",
    "Below do exactly as above with Adaptive boosting but now with gradient boosting. Unfortunately you will not be able to use ExtraTrees here so you will end up just gradient boosting the default Decision Tree classifier. Again report the results and as before do 5 fold cross valitation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Voting classifier\n",
    "\n",
    "Here you are going to implement a card voting classifier. You will pick three diverse learners of your choice. For a group of learners to be diverse means that they make different errors on the same data. In other words, one might say that the errors made by the classifers are uncorrelated.\n",
    "\n",
    "Use sklearn's VotingClassifier to bind them together and again use cross_validate to score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Write a conclusion explaining what we should take away from the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowlegement: Many Thanks to Toma Suciu for the first draft**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
